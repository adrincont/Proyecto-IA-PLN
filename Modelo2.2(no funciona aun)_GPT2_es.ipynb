{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHkYFOFNvqi-"
      },
      "source": [
        "# **Modelo 2 (Generacion de Poemas) GPT2**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers -U"
      ],
      "metadata": {
        "id": "WGXNdxfBLrwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SAgacGzvqjB"
      },
      "outputs": [],
      "source": [
        "# Basicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "# Texto\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import get_scheduler\n",
        "import random\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import pipeline\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Funciones y variables\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datos**"
      ],
      "metadata": {
        "id": "J1yX23FRe0yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/andreamorgar/poesIA/master/data/poems.csv'\n",
        "poems_df = pd.read_csv(url)\n",
        "poems_df = poems_df.dropna()"
      ],
      "metadata": {
        "id": "1YLm8TDBezt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar poemas grandes\n",
        "poems_df['string'] = poems_df.apply(lambda row: f'\\n{row[\"title\"]}\\n{row[\"content\"]}', axis=1)\n",
        "poems_df['length'] = poems_df.string.map(len)\n",
        "MAX_POEM_LENGTH=500\n",
        "poems_filtered = poems_df[poems_df.length<MAX_POEM_LENGTH]\n",
        "_ , poems_filtered = train_test_split(poems_filtered, test_size = 0.9 ,shuffle=True)\n",
        "poems_filtered"
      ],
      "metadata": {
        "id": "1g4lr7TxghDE",
        "outputId": "da3197a7-668d-4b35-fa85-b1d4da6dccc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       author  \\\n",
              "3420              Amado Nervo   \n",
              "4687         Oliverio Girondo   \n",
              "2113  Genaro Ortega Gutiérrez   \n",
              "3046       Alejandra Pizarnik   \n",
              "3010       Juan Ramón Jiménez   \n",
              "...                       ...   \n",
              "1106          Mario Benedetti   \n",
              "2451            Alfredo Buxán   \n",
              "3268            Nicanor Parra   \n",
              "2559         José Ángel Buesa   \n",
              "3056       Juan Ramón Jiménez   \n",
              "\n",
              "                                                content  \\\n",
              "3420  \\n¿Versos autobiográficos ? Ahí están mis canc...   \n",
              "4687  \\n\\nMenos rodante dado\\ndeliquio sumo psíquico...   \n",
              "2113  Obligados a abandonar\\nmuchos sueños ya rotos ...   \n",
              "3046  \\n\\nEn el eco de mis muertes\\naún hay miedo.\\n...   \n",
              "3010  \\n\\nSólo lo hiciste un momento.\\nMas quedaste,...   \n",
              "...                                                 ...   \n",
              "1106  De un tiempo a esta parte\\nel infinito\\nse ha ...   \n",
              "2451  ¿Qué bien echas en falta si respiras,\\nsi cuel...   \n",
              "3268  \\n\\n         1\\n\\nYa no me queda nada por deci...   \n",
              "2559  \\n\\nUn gran amor, un gran amor lejano\\nes algo...   \n",
              "3056  \\n\\nSilencio. Sólo queda\\nun olor de jazmín.\\n...   \n",
              "\n",
              "                         title  \\\n",
              "3420             Autobiografía   \n",
              "4687                     MENOS   \n",
              "2113  Vida íntima de la pleura   \n",
              "3046                  EL MIEDO   \n",
              "3010                LA ACTITUD   \n",
              "...                        ...   \n",
              "1106               El infinito   \n",
              "2451              El resentido   \n",
              "3268              TRES POESÍAS   \n",
              "2559              EL GRAN AMOR   \n",
              "3056             PATIO PRIMERO   \n",
              "\n",
              "                                                 string  length  \n",
              "3420  \\nAutobiografía\\n\\n¿Versos autobiográficos ? A...     468  \n",
              "4687  \\nMENOS\\n\\n\\nMenos rodante dado\\ndeliquio sumo...     314  \n",
              "2113  \\nVida íntima de la pleura\\nObligados a abando...     429  \n",
              "3046  \\nEL MIEDO\\n\\n\\nEn el eco de mis muertes\\naún ...     276  \n",
              "3010  \\nLA ACTITUD\\n\\n\\nSólo lo hiciste un momento.\\...      96  \n",
              "...                                                 ...     ...  \n",
              "1106  \\nEl infinito\\nDe un tiempo a esta parte\\nel i...     211  \n",
              "2451  \\nEl resentido\\n¿Qué bien echas en falta si re...     322  \n",
              "3268  \\nTRES POESÍAS\\n\\n\\n         1\\n\\nYa no me que...     369  \n",
              "2559  \\nEL GRAN AMOR\\n\\n\\nUn gran amor, un gran amor...     432  \n",
              "3056  \\nPATIO PRIMERO\\n\\n\\nSilencio. Sólo queda\\nun ...     129  \n",
              "\n",
              "[1236 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f06ee87-43b6-451c-8451-05ec3b18a4d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "      <th>title</th>\n",
              "      <th>string</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3420</th>\n",
              "      <td>Amado Nervo</td>\n",
              "      <td>\\n¿Versos autobiográficos ? Ahí están mis canc...</td>\n",
              "      <td>Autobiografía</td>\n",
              "      <td>\\nAutobiografía\\n\\n¿Versos autobiográficos ? A...</td>\n",
              "      <td>468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4687</th>\n",
              "      <td>Oliverio Girondo</td>\n",
              "      <td>\\n\\nMenos rodante dado\\ndeliquio sumo psíquico...</td>\n",
              "      <td>MENOS</td>\n",
              "      <td>\\nMENOS\\n\\n\\nMenos rodante dado\\ndeliquio sumo...</td>\n",
              "      <td>314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2113</th>\n",
              "      <td>Genaro Ortega Gutiérrez</td>\n",
              "      <td>Obligados a abandonar\\nmuchos sueños ya rotos ...</td>\n",
              "      <td>Vida íntima de la pleura</td>\n",
              "      <td>\\nVida íntima de la pleura\\nObligados a abando...</td>\n",
              "      <td>429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3046</th>\n",
              "      <td>Alejandra Pizarnik</td>\n",
              "      <td>\\n\\nEn el eco de mis muertes\\naún hay miedo.\\n...</td>\n",
              "      <td>EL MIEDO</td>\n",
              "      <td>\\nEL MIEDO\\n\\n\\nEn el eco de mis muertes\\naún ...</td>\n",
              "      <td>276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3010</th>\n",
              "      <td>Juan Ramón Jiménez</td>\n",
              "      <td>\\n\\nSólo lo hiciste un momento.\\nMas quedaste,...</td>\n",
              "      <td>LA ACTITUD</td>\n",
              "      <td>\\nLA ACTITUD\\n\\n\\nSólo lo hiciste un momento.\\...</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106</th>\n",
              "      <td>Mario Benedetti</td>\n",
              "      <td>De un tiempo a esta parte\\nel infinito\\nse ha ...</td>\n",
              "      <td>El infinito</td>\n",
              "      <td>\\nEl infinito\\nDe un tiempo a esta parte\\nel i...</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2451</th>\n",
              "      <td>Alfredo Buxán</td>\n",
              "      <td>¿Qué bien echas en falta si respiras,\\nsi cuel...</td>\n",
              "      <td>El resentido</td>\n",
              "      <td>\\nEl resentido\\n¿Qué bien echas en falta si re...</td>\n",
              "      <td>322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3268</th>\n",
              "      <td>Nicanor Parra</td>\n",
              "      <td>\\n\\n         1\\n\\nYa no me queda nada por deci...</td>\n",
              "      <td>TRES POESÍAS</td>\n",
              "      <td>\\nTRES POESÍAS\\n\\n\\n         1\\n\\nYa no me que...</td>\n",
              "      <td>369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2559</th>\n",
              "      <td>José Ángel Buesa</td>\n",
              "      <td>\\n\\nUn gran amor, un gran amor lejano\\nes algo...</td>\n",
              "      <td>EL GRAN AMOR</td>\n",
              "      <td>\\nEL GRAN AMOR\\n\\n\\nUn gran amor, un gran amor...</td>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>Juan Ramón Jiménez</td>\n",
              "      <td>\\n\\nSilencio. Sólo queda\\nun olor de jazmín.\\n...</td>\n",
              "      <td>PATIO PRIMERO</td>\n",
              "      <td>\\nPATIO PRIMERO\\n\\n\\nSilencio. Sólo queda\\nun ...</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1236 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f06ee87-43b6-451c-8451-05ec3b18a4d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f06ee87-43b6-451c-8451-05ec3b18a4d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f06ee87-43b6-451c-8451-05ec3b18a4d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(poems_filtered['string'])[20])"
      ],
      "metadata": {
        "id": "QRpHDSqjhS56",
        "outputId": "4abb901d-5c99-40b8-80e9-1519c6e48b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NO PUEDE\n",
            "\n",
            "\n",
            "No puede conmigo\n",
            "la tristeza\n",
            "la arrastro hacia la vida\n",
            "y se evapora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokens para los datos (modelo DeepESP/gpt2-spanish)**"
      ],
      "metadata": {
        "id": "F2gZ7FTsiC70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Informacion del modelo ------------------------------------------------------\\\n",
        "df = poems_filtered['string'] # Datos\n",
        "max_length = 800 # Longitud maxima de los poemas\n",
        "modelo_gpt = \"DeepESP/gpt2-spanish\" # Modelo pre entrenado\n",
        "RANDOM_SEED = 73 # Semilla"
      ],
      "metadata": {
        "id": "O7yoYPOlM3Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(modelo_gpt)\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# Tokenizador del modelo ------------------------------------------------------\\\n",
        "class DataTokens(Dataset):\n",
        "  def __init__(self, data, tokenizer, gpt2_type=\"gpt2\", max_length=max_length):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    for row in data:\n",
        "      self.encodings_dict = self.tokenizer('<BOS>' + row + '<EOS>', padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "      self.input_ids.append(torch.tensor(self.encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(self.encodings_dict['attention_mask']))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "# Clase de los datos ----------------------------------------------------------\\\n",
        "class DataModule():\n",
        "  # Definimos un tamaño de lote en la clase\n",
        "  def __init__(self, dataset, tokenizer, gpt2_type=\"gpt2\", batch_size = 32, p = 0.8):\n",
        "      super(DataModule,self).__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.dataset = dataset\n",
        "      self.tokenizer = tokenizer\n",
        "      self.p = p\n",
        "  # Definimos el tratamiento de los datos\n",
        "  def train_val_split(self, split, dataset):\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size\n",
        "  def setup(self, stage=None):\n",
        "    self.dataset = DataTokens(self.dataset, self.tokenizer, gpt2_type=gpt2_type)\n",
        "    train_size, val_size = self.train_val_split(self.p, self.dataset)\n",
        "    self.train_dataset, self.val_dataset = random_split(self.dataset, [train_size, val_size])\n",
        "  # Iterable de entrenamiento\n",
        "  def train_dataloader(self):\n",
        "      return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "  # Iterable de validacion\n",
        "  def val_dataloader(self):\n",
        "      return DataLoader(self.val_dataset, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "baMTYb6qxl7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reentrenamiento para el modelo (DeepESP/gpt2-spanish)**"
      ],
      "metadata": {
        "id": "Vw0DxarOic-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fijar semillas --------------------------------------------------------------\\\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "9-XZRfiNsyDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo ----------------------------------------------------\\\n",
        "class Trainer_poet():\n",
        "    def __init__(self, dataset, model, batch_size=16, epochs=5, learning_rate = 1e-4, eps = 1e-8, warmup_steps=50):\n",
        "      # DataLoaders\n",
        "      self.data_loader = DataModule(dataset, batch_size = batch_size, p = 0.8)\n",
        "      data_loader.setup()\n",
        "      self.train_dataloader = data_loader.train_dataloader()\n",
        "      self.val_dataloader = data_loader.val_dataloader()\n",
        "      # Modelo\n",
        "      self.model = model\n",
        "      self.epochs = epochs\n",
        "      self.batch_size = batch_size\n",
        "      self.optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\n",
        "      total_steps = len(train_dataloader) * epochs\n",
        "      self.scheduler = get_scheduler(name='linear',optimizer=optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps)\n",
        "    def train(self):\n",
        "      device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "      model = self.model\n",
        "      model.to(device)\n",
        "      model.train()\n",
        "      start_time = time.time()\n",
        "      # Entrenamiento\n",
        "      torch.cuda.empty_cache()\n",
        "      print('Inicio entrenamiento ....')\n",
        "      for epoch_i in range(self.epochs):\n",
        "        print(f'Epoch {epoch_i + 1} de {epochs}')\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        for step, batch in enumerate(self.train_dataloader):\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_masks = batch[1].to(device)\n",
        "          outputs = model(b_input_ids,labels=b_input_ids,attention_mask=b_masks)\n",
        "          loss = outputs[0]\n",
        "          batch_loss = loss.item()\n",
        "          total_train_loss += batch_loss\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "        avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        print(f'Average Training Loss: {avg_train_loss}. Epoch Training Time: {training_time}')\n",
        "        # Validacion\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        for batch in self.val_dataloader:\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_masks = batch[1].to(device)\n",
        "          with torch.no_grad():\n",
        "            outputs  = model(b_input_ids,attention_mask=b_masks,labels=b_input_ids)\n",
        "            loss = outputs[0]\n",
        "          batch_loss = loss.item()\n",
        "          total_eval_loss += batch_loss\n",
        "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "        print(f'Average Validation Loss: {avg_val_loss}')\n",
        "      self.total_train_loss = total_train_loss\n",
        "      self.total_eval_loss = total_eval_loss\n",
        "      self.model = model\n",
        "      print(f'Total Training Time: {format_time(time.time()-start_time)}')\n",
        "      return model"
      ],
      "metadata": {
        "id": "Vu3otaqQO0kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpt2_esp = GPT2LMHeadModel.from_pretrained(modelo_gpt, pad_token_id=tokenizer.eos_token_id)\n",
        "Trainer_model = Trainer_poet(df, model_gpt2_esp, epochs=50)\n",
        "model = Trainer_model.train()\n",
        "torch.save(model, 'modelo_gpt2_poesia.pt')"
      ],
      "metadata": {
        "id": "x-bpHQUuRVnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generación de Poesía**"
      ],
      "metadata": {
        "id": "8Tl7dP3niubb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('modelo_gpt2_poesia.pt')"
      ],
      "metadata": {
        "id": "AKftdGuSRz-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,tokenizer,prompt,length=60,top_p=0.8,temperature=1.):\n",
        "    #prompt = trad_es_en(prompt)[0]['translation_text']\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    filter_value = -float(\"Inf\")\n",
        "    with torch.no_grad():\n",
        "      entry_finished = False\n",
        "      generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "      for i in range(length):\n",
        "          outputs = model(generated, labels=generated)\n",
        "          loss, logits = outputs[:2]\n",
        "          logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "          sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "          cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "          sorted_indices_to_remove = cumulative_probs > top_p\n",
        "          sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "          sorted_indices_to_remove[..., 0] = 0\n",
        "          indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "          logits[:, indices_to_remove] = filter_value\n",
        "          next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "          generated = torch.cat((generated, next_token), dim=1)\n",
        "          if next_token in tokenizer.encode(\"<|endoftext|>\"): break \n",
        "      output_list = list(generated.squeeze().numpy())\n",
        "      output_text = tokenizer.decode(output_list,skip_special_tokens=True)\n",
        "      generated_list.append(output_text)\n",
        "    #generated_list = trad_en_es(generated_list)[0]['translation_text']  \n",
        "    return generated_list[0]"
      ],
      "metadata": {
        "id": "kqsCgwqCx4z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'CIELO ESTRELLADO'\n",
        "text = generate(model.to('cpu'), tokenizer,Palabra,temperature=0.7,length = 100,top_p = 0.8)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "wLO6WZwJ2_zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generate2(model,Palabra):\n",
        "  input_ids = tokenizer.encode(Palabra, return_tensors=\"pt\")\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      do_sample=True,\n",
        "      top_k=50,\n",
        "      max_length=40,\n",
        "      top_p=0.95,\n",
        "      num_return_sequences=3,\n",
        "      #temperature=1.5\n",
        "      #no_repeat_ngram_size=2,\n",
        "      #early_stopping=True,\n",
        "      #num_beams=5\n",
        "  )\n",
        "  output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "metadata": {
        "id": "tUxkbcRVP2Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'CIELO ESTRELLADO'\n",
        "text = Generate2(model.to('cpu'),Palabra)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "SQQ_9VMaSEL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "94e7570899995adebab4ebab5cd3752e227f734c99b4f5f3f0d280f8bef09b63"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('IA')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Gasolina.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}