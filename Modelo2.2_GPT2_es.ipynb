{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHkYFOFNvqi-"
      },
      "source": [
        "# **Modelo 2 (Generacion de Poemas) GPT2**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers -U"
      ],
      "metadata": {
        "id": "WGXNdxfBLrwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc042a80-667a-45c5-fd2d-cd38510576c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2SAgacGzvqjB"
      },
      "outputs": [],
      "source": [
        "# Basicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "# Texto\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import get_scheduler\n",
        "import random\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import pipeline\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Funciones y variables\n",
        "def format_time(elapsed): return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Informacion del modelo ------------------------------------------------------\\\n",
        "max_length = 800 # Longitud maxima de los poemas\n",
        "modelo_gpt = \"DeepESP/gpt2-spanish\" # Modelo pre entrenado\n",
        "RANDOM_SEED = 2022 # Semilla"
      ],
      "metadata": {
        "id": "239Pi91Mv66B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datos**"
      ],
      "metadata": {
        "id": "J1yX23FRe0yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/andreamorgar/poesIA/master/data/poems.csv'\n",
        "poems_df = pd.read_csv(url)\n",
        "poems_df = poems_df.dropna()"
      ],
      "metadata": {
        "id": "1YLm8TDBezt-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar poemas grandes\n",
        "poems_df['string'] = poems_df.apply(lambda row: f'\\n{row[\"title\"]}\\n{row[\"content\"]}', axis=1)\n",
        "poems_df['length'] = poems_df.string.map(len)\n",
        "poems_filtered = poems_df[poems_df.length < max_length]\n",
        "_ , poems_filtered = train_test_split(poems_filtered, test_size = 0.9 ,shuffle=True,random_state = 2022)\n",
        "poems_filtered"
      ],
      "metadata": {
        "id": "1g4lr7TxghDE",
        "outputId": "2df5fdb9-54fe-4fff-e4b1-e5839070ed7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      author  \\\n",
              "2544   Francisco de Figueroa   \n",
              "908          Luis de Góngora   \n",
              "5029    José Gautier Benítez   \n",
              "3942          Víctor Jiménez   \n",
              "3927  Gabriel García Márquez   \n",
              "...                      ...   \n",
              "479           Jorge Teillier   \n",
              "4052      Juan Ramón Jiménez   \n",
              "299             Víctor Botas   \n",
              "2065       Toni García Arias   \n",
              "1363        Claribel Alegría   \n",
              "\n",
              "                                                content  \\\n",
              "2544  \\n\\nPerdido ando, señora, entre la gente,\\nsin...   \n",
              "908   \\n\\n¡Oh, de alto valor, de virtud rara\\nSacro ...   \n",
              "5029  \\n\\nCuando no reste ya ni un solo grano\\nde mi...   \n",
              "3942  A la ausencia, al olvido, a la nostalgia\\nmi c...   \n",
              "3927  Si alguien llama a tu puerta, amiga mía,\\ny al...   \n",
              "...                                                 ...   \n",
              "479   \\n\\nSentados frente al fuego que envejece\\nmir...   \n",
              "4052  \\n\\n¡Qué miedo el azul del cielo!\\n¡Negro!\\n¡N...   \n",
              "299   No me preguntes cómo pasa el tiempo\\nLi Kiu Li...   \n",
              "2065  Decías unas cosas que me asustaban.\\nEn cubier...   \n",
              "1363  \\n\\nFui la nube\\ny la lluvia\\ny el mar\\ny quie...   \n",
              "\n",
              "                                 title  \\\n",
              "2544                       SONETO XVII   \n",
              "908              A DON ANTONIO VENEGAS   \n",
              "5029                      A MIS AMIGOS   \n",
              "3942  Tango para engañar a la tristeza   \n",
              "3927      Si alguien llama a tu puerta   \n",
              "...                                ...   \n",
              "479           SENTADOS FRENTE AL FUEGO   \n",
              "4052          TRASCIELO DEL CIELO AZUL   \n",
              "299             Las rosas de Babilonia   \n",
              "2065                 Sobre la cubierta   \n",
              "1363                         FRONTERAS   \n",
              "\n",
              "                                                 string  length  \n",
              "2544  \\nSONETO XVII\\n\\n\\nPerdido ando, señora, entre...     558  \n",
              "908   \\nA DON ANTONIO VENEGAS\\n\\n\\n¡Oh, de alto valo...     511  \n",
              "5029  \\nA MIS AMIGOS\\n\\n\\nCuando no reste ya ni un s...     599  \n",
              "3942  \\nTango para engañar a la tristeza\\nA la ausen...     272  \n",
              "3927  \\nSi alguien llama a tu puerta\\nSi alguien lla...     544  \n",
              "...                                                 ...     ...  \n",
              "479   \\nSENTADOS FRENTE AL FUEGO\\n\\n\\nSentados frent...     703  \n",
              "4052  \\nTRASCIELO DEL CIELO AZUL\\n\\n\\n¡Qué miedo el ...     267  \n",
              "299   \\nLas rosas de Babilonia\\nNo me preguntes cómo...     703  \n",
              "2065  \\nSobre la cubierta\\nDecías unas cosas que me ...     372  \n",
              "1363  \\nFRONTERAS\\n\\n\\nFui la nube\\ny la lluvia\\ny e...      86  \n",
              "\n",
              "[2961 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9b48b2c-4fcb-4392-98a1-9e8f77642b7f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "      <th>title</th>\n",
              "      <th>string</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2544</th>\n",
              "      <td>Francisco de Figueroa</td>\n",
              "      <td>\\n\\nPerdido ando, señora, entre la gente,\\nsin...</td>\n",
              "      <td>SONETO XVII</td>\n",
              "      <td>\\nSONETO XVII\\n\\n\\nPerdido ando, señora, entre...</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>Luis de Góngora</td>\n",
              "      <td>\\n\\n¡Oh, de alto valor, de virtud rara\\nSacro ...</td>\n",
              "      <td>A DON ANTONIO VENEGAS</td>\n",
              "      <td>\\nA DON ANTONIO VENEGAS\\n\\n\\n¡Oh, de alto valo...</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5029</th>\n",
              "      <td>José Gautier Benítez</td>\n",
              "      <td>\\n\\nCuando no reste ya ni un solo grano\\nde mi...</td>\n",
              "      <td>A MIS AMIGOS</td>\n",
              "      <td>\\nA MIS AMIGOS\\n\\n\\nCuando no reste ya ni un s...</td>\n",
              "      <td>599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3942</th>\n",
              "      <td>Víctor Jiménez</td>\n",
              "      <td>A la ausencia, al olvido, a la nostalgia\\nmi c...</td>\n",
              "      <td>Tango para engañar a la tristeza</td>\n",
              "      <td>\\nTango para engañar a la tristeza\\nA la ausen...</td>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3927</th>\n",
              "      <td>Gabriel García Márquez</td>\n",
              "      <td>Si alguien llama a tu puerta, amiga mía,\\ny al...</td>\n",
              "      <td>Si alguien llama a tu puerta</td>\n",
              "      <td>\\nSi alguien llama a tu puerta\\nSi alguien lla...</td>\n",
              "      <td>544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>Jorge Teillier</td>\n",
              "      <td>\\n\\nSentados frente al fuego que envejece\\nmir...</td>\n",
              "      <td>SENTADOS FRENTE AL FUEGO</td>\n",
              "      <td>\\nSENTADOS FRENTE AL FUEGO\\n\\n\\nSentados frent...</td>\n",
              "      <td>703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4052</th>\n",
              "      <td>Juan Ramón Jiménez</td>\n",
              "      <td>\\n\\n¡Qué miedo el azul del cielo!\\n¡Negro!\\n¡N...</td>\n",
              "      <td>TRASCIELO DEL CIELO AZUL</td>\n",
              "      <td>\\nTRASCIELO DEL CIELO AZUL\\n\\n\\n¡Qué miedo el ...</td>\n",
              "      <td>267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>Víctor Botas</td>\n",
              "      <td>No me preguntes cómo pasa el tiempo\\nLi Kiu Li...</td>\n",
              "      <td>Las rosas de Babilonia</td>\n",
              "      <td>\\nLas rosas de Babilonia\\nNo me preguntes cómo...</td>\n",
              "      <td>703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2065</th>\n",
              "      <td>Toni García Arias</td>\n",
              "      <td>Decías unas cosas que me asustaban.\\nEn cubier...</td>\n",
              "      <td>Sobre la cubierta</td>\n",
              "      <td>\\nSobre la cubierta\\nDecías unas cosas que me ...</td>\n",
              "      <td>372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1363</th>\n",
              "      <td>Claribel Alegría</td>\n",
              "      <td>\\n\\nFui la nube\\ny la lluvia\\ny el mar\\ny quie...</td>\n",
              "      <td>FRONTERAS</td>\n",
              "      <td>\\nFRONTERAS\\n\\n\\nFui la nube\\ny la lluvia\\ny e...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2961 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9b48b2c-4fcb-4392-98a1-9e8f77642b7f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9b48b2c-4fcb-4392-98a1-9e8f77642b7f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9b48b2c-4fcb-4392-98a1-9e8f77642b7f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(poems_filtered['string'])[20])"
      ],
      "metadata": {
        "id": "QRpHDSqjhS56",
        "outputId": "01522994-8415-4608-d823-0c9dd37b0faa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Llegué a Valladolid; registré luego\n",
            "\n",
            "\n",
            "Llegué a Valladolid; registré luego\n",
            "Desde el bonete al clavo de la mula;\n",
            "Guardo el registro, que será mi bula\n",
            "Contra el cuidado del señor don Diego.\n",
            "\n",
            "Busqué la Corte en él, y yo estoy ciego,\n",
            "O en la ciudad no está, o se disimula.\n",
            "Celebrando dïetas vi a la gula,\n",
            "Que Platón para todos está en griego.\n",
            "\n",
            "La lisonja hallé y la ceremonia\n",
            "Con luto, idolatrados los caciques,\n",
            "Amor sin fe, interés con sus virotes.\n",
            "\n",
            "Todo se halla en esta Babilonia,\n",
            "Como en botica, grandes alambiques,\n",
            "Y más en ella títulos que botes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokens para los datos (modelo DeepESP/gpt2-spanish)**"
      ],
      "metadata": {
        "id": "F2gZ7FTsiC70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = poems_filtered['string'] # Datos\n",
        "# Tokenizador del modelo pre entrenado ----------------------------------------\\\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(modelo_gpt)\n",
        "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "metadata": {
        "id": "z8C56b9OqUPG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizador del modelo ------------------------------------------------------\\\n",
        "class DataTokens(Dataset):\n",
        "  def __init__(self, data, tokenizer, gpt2_type=\"gpt2\", max_length=max_length):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    for row in data:\n",
        "      self.encodings_dict = self.tokenizer('<BOS>' + row + '<EOS>', padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "      self.input_ids.append(torch.tensor(self.encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(self.encodings_dict['attention_mask']))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "# Clase de los datos ----------------------------------------------------------\\\n",
        "class DataModule():\n",
        "  # Definimos un tamaño de lote en la clase\n",
        "  def __init__(self, dataset, tokenizer, gpt2_type=\"gpt2\", p = 0.8):\n",
        "      super(DataModule,self).__init__()\n",
        "      self.dataset = dataset\n",
        "      self.tokenizer = tokenizer\n",
        "      self.p = p\n",
        "      self.gpt2_type = gpt2_type\n",
        "  # Definimos el tratamiento de los datos\n",
        "  def train_val_split(self, split, dataset):\n",
        "    train_size = int(split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    return train_size, val_size\n",
        "  def setup(self, stage=None):\n",
        "    self.dataset = DataTokens(self.dataset, self.tokenizer, gpt2_type=self.gpt2_type)\n",
        "    train_size, val_size = self.train_val_split(self.p, self.dataset)\n",
        "    self.train_dataset, self.val_dataset = random_split(self.dataset, [train_size, val_size])\n",
        "  # Iterable de entrenamiento\n",
        "  def train_dataloader(self, batch_size = 32):\n",
        "      return torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size)\n",
        "  # Iterable de validacion\n",
        "  def val_dataloader(self, batch_size = 32):\n",
        "      return torch.utils.data.DataLoader(self.val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "baMTYb6qxl7o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reentrenamiento para el modelo (DeepESP/gpt2-spanish)**"
      ],
      "metadata": {
        "id": "Vw0DxarOic-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fijar semillas --------------------------------------------------------------\\\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "9-XZRfiNsyDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c3dc0a-b660-4cf9-c1a3-e46f2a962971"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa8c62197b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor[0].size()[1] + packed_tensor[0].size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = [torch.cat([new_tensor[0], packed_tensor[0][:, 1:]], dim=1)\n",
        "          ,torch.cat([new_tensor[1], packed_tensor[1][:, 1:]], dim=1)]\n",
        "        return packed_tensor, True, None\n",
        "# Entrenamiento del modelo ----------------------------------------------------\\\n",
        "class Trainer_poet():\n",
        "    def __init__(self, dataset, model, batch_size=16, epochs=5, learning_rate = 1e-4, eps = 1e-8, warmup_steps=50):\n",
        "      # DataLoaders\n",
        "      self.data_loader = dataset\n",
        "      self.data_loader.setup()\n",
        "      self.train_dataloader = self.data_loader.train_dataloader(batch_size = 1)\n",
        "      self.val_dataloader = self.data_loader.val_dataloader(batch_size = 1)\n",
        "      # Modelo\n",
        "      self.model = model\n",
        "      self.batch_size = batch_size\n",
        "      self.epochs = epochs\n",
        "      self.optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\n",
        "      total_steps = len(self.train_dataloader) * epochs\n",
        "      self.scheduler = get_linear_schedule_with_warmup(optimizer=self.optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps)\n",
        "    def train(self):\n",
        "      device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "      model = self.model\n",
        "      optimizer =  self.optimizer\n",
        "      scheduler = self.scheduler\n",
        "      model.to(device)\n",
        "      model.train()\n",
        "      start_time = time.time()\n",
        "      training_stats = []\n",
        "      # Entrenamiento\n",
        "      print('Inicio entrenamiento ....')\n",
        "      train_dataloader = self.train_dataloader\n",
        "      val_dataloader = self.val_dataloader\n",
        "      for epoch_i in range(self.epochs):\n",
        "        print(f'Epoch {epoch_i + 1} de {self.epochs}')\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        input_tensor = None\n",
        "        accumulating_batch_count = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "          (input_tensor, carry_on, remainder) = pack_tensor(batch, input_tensor, 768)\n",
        "          if carry_on and step != len(train_dataloader) - 1: continue\n",
        "\n",
        "          b_input_ids = input_tensor[0].to(device)\n",
        "          b_masks = input_tensor[1].to(device)\n",
        "          input_tensor = [b_input_ids, b_masks]\n",
        "          outputs = model(b_input_ids,labels=b_input_ids,attention_mask=b_masks)\n",
        "          loss = outputs[0]\n",
        "          loss.backward()\n",
        "          if (accumulating_batch_count % self.batch_size) == 0:\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "          accumulating_batch_count += 1\n",
        "          input_tensor = None\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        print(f'Average Training Loss: {avg_train_loss}. Epoch Training Time: {training_time}')\n",
        "        # Validacion\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        for batch in val_dataloader:\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_masks = batch[1].to(device)\n",
        "          with torch.no_grad():\n",
        "            outputs  = model(b_input_ids,attention_mask=b_masks,labels=b_input_ids)\n",
        "            loss = outputs[0]\n",
        "          batch_loss = loss.item()\n",
        "          total_eval_loss += batch_loss\n",
        "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "        validation_time = format_time(time.time() - t0) \n",
        "        print(f'Average Validation Loss: {avg_val_loss}')\n",
        "        # Guardar estadisticas\n",
        "        training_stats.append(\n",
        "            {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "             }\n",
        "          )\n",
        "      self.training_stats = training_stats\n",
        "      self.model = model\n",
        "      print(f'Total Training Time: {format_time(time.time()-start_time)}')\n",
        "      return model"
      ],
      "metadata": {
        "id": "Vu3otaqQO0kO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=max_length).from_pretrained(modelo_gpt, output_hidden_states=True)\n",
        "model_gpt2_esp = GPT2LMHeadModel.from_pretrained(modelo_gpt, config=configuration)\n",
        "model_gpt2_esp.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "Dataset = DataModule(df, tokenizer, gpt2_type=modelo_gpt)\n",
        "Trainer_model = Trainer_poet(Dataset, model_gpt2_esp, epochs=10, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4PX7_szsFOT",
        "outputId": "52089d99-22e6-4e9a-b8ce-a73519aeb34a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Trainer_model.train()\n",
        "torch.save(model, 'modelo_gpt2_poesia.pt')"
      ],
      "metadata": {
        "id": "x-bpHQUuRVnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f645a2e2-37b2-4d92-b93b-4ad03cd8a065"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicio entrenamiento ....\n",
            "Epoch 1 de 10\n",
            "Average Training Loss: 0.054826443357317636. Epoch Training Time: 0:02:55\n",
            "Average Validation Loss: 0.7208625375721949\n",
            "Epoch 2 de 10\n",
            "Average Training Loss: 0.021068227584347933. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.6930688514702437\n",
            "Epoch 3 de 10\n",
            "Average Training Loss: 0.019718704691685334. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.6864213045642509\n",
            "Epoch 4 de 10\n",
            "Average Training Loss: 0.01870843443415455. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.6860056349011939\n",
            "Epoch 5 de 10\n",
            "Average Training Loss: 0.017727699577556672. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.68893584291157\n",
            "Epoch 6 de 10\n",
            "Average Training Loss: 0.016660080785309343. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.6952692445926683\n",
            "Epoch 7 de 10\n",
            "Average Training Loss: 0.015499611866559732. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.7046269372256369\n",
            "Epoch 8 de 10\n",
            "Average Training Loss: 0.014228151349753543. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.7174973511726016\n",
            "Epoch 9 de 10\n",
            "Average Training Loss: 0.01281023202180812. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.7331889093072981\n",
            "Epoch 10 de 10\n",
            "Average Training Loss: 0.01118943587455244. Epoch Training Time: 0:02:51\n",
            "Average Validation Loss: 0.7527769932155867\n",
            "Total Training Time: 0:33:26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generación de Poesía**"
      ],
      "metadata": {
        "id": "8Tl7dP3niubb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('modelo_gpt2_poesia.pt')"
      ],
      "metadata": {
        "id": "AKftdGuSRz-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,tokenizer,prompt,length=60,top_p=0.8,temperature=1.):\n",
        "    #prompt = trad_es_en(prompt)[0]['translation_text']\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    filter_value = -float(\"Inf\")\n",
        "    with torch.no_grad():\n",
        "      entry_finished = False\n",
        "      generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "      for i in range(length):\n",
        "          outputs = model(generated, labels=generated)\n",
        "          loss, logits = outputs[:2]\n",
        "          logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "          sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "          cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "          sorted_indices_to_remove = cumulative_probs > top_p\n",
        "          sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "          sorted_indices_to_remove[..., 0] = 0\n",
        "          indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "          logits[:, indices_to_remove] = filter_value\n",
        "          next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "          generated = torch.cat((generated, next_token), dim=1)\n",
        "          if next_token in tokenizer.encode(\"<|endoftext|>\"): break \n",
        "      output_list = list(generated.squeeze().numpy())\n",
        "      output_text = tokenizer.decode(output_list,skip_special_tokens=True)\n",
        "      generated_list.append(output_text)\n",
        "    #generated_list = trad_en_es(generated_list)[0]['translation_text']  \n",
        "    return generated_list[0]"
      ],
      "metadata": {
        "id": "kqsCgwqCx4z1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'MI PEQUEÑA'\n",
        "text = generate(model.to('cpu'), tokenizer,Palabra,temperature=0.7,length = 100,top_p = 0.8)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "wLO6WZwJ2_zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9625b3-e2dd-433d-d47d-2c9cfedcdf7e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MI PEQUEÑA\n",
            "\n",
            "\n",
            "A Federico,\n",
            "por quien me has tomado\n",
            "\n",
            "el corazón,\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "\n",
            "por quien me has tomado\n",
            "el corazón,\n",
            "\n",
            "\n",
            "por quien me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Generate2(model,Palabra):\n",
        "  input_ids = tokenizer.encode(Palabra, return_tensors=\"pt\")\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      do_sample=True,\n",
        "      top_k=50,\n",
        "      max_length=200,\n",
        "      top_p=0.95,\n",
        "      num_return_sequences=1,\n",
        "      #temperature=1.5\n",
        "      #no_repeat_ngram_size=2,\n",
        "      #early_stopping=True,\n",
        "      #num_beams=5\n",
        "  )\n",
        "  output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "metadata": {
        "id": "tUxkbcRVP2Fn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'MI MUERTE'\n",
        "text = Generate2(model.to('cpu'),Palabra)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "SQQ_9VMaSEL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0875f12-e21a-4d8a-81ba-16499b67c039"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MI MUERTE\n",
            "\n",
            "La tarde caía\n",
            "los árboles derribados\n",
            "en el polvo.\n",
            "\n",
            "Conmovidos por el cansancio,\n",
            "al alba, el Sena\n",
            "se abría paso entre la bruma\n",
            "derramando.\n",
            "\n",
            "El agua, ya casi fría,\n",
            "turba el agua de la Gran lluvia.\n",
            "\n",
            "Se apagaba la lámpara\n",
            "y, a cada nuevo relámpago,\n",
            "el crepúsculo... \n",
            "\n",
            "Me hacía la oscuridad un sueño,\n",
            "una agonía\n",
            "cada nuevo heraldo,\n",
            "cua que dijera en la noche\n",
            "que yo era la noche. nunca... nunca...\n",
            "Me ardían los oídos\n",
            "la noche entera. siempre...\n",
            "... una vez más, el mundo\n",
            "en torno a mí se perdía,\n",
            "y la noche, en el tiempo, una llama. siempre... nunca...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'TERMINAR'\n",
        "text = Generate2(model.to('cpu'),Palabra)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "qkG_ItxypMIi",
        "outputId": "36affc16-3e26-49ca-a1d2-7fe042562c58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TERMINAR\n",
            "\n",
            "\n",
            "De su hermosura fina\n",
            "la luz revela;\n",
            "el alma olvida,\n",
            "porque es luz de cielo;\n",
            "y en vano la luz olvida.,\n",
            "\n",
            "La hermosura me revela;\n",
            "es luz de cielo,\n",
            "de sol sin tasa fijo,\n",
            "de sol sin tasa fijo\n",
            "aún oscuro,\n",
            "de luz a noche;\n",
            "\n",
            "¿en vano la luz me revela,\n",
            "y a qué precio?\n",
            "\n",
            "Sucede que a tales cosas\n",
            "\n",
            "No digo, señora, que no le amargue la luz,\n",
            "que es gran bien;\n",
            "mas quisiera yo a su hermosura\n",
            "olvidarme,\n",
            "mas quisiera a mi alma.,\n",
            "la luz me revela;\n",
            "y el alma me deja, me hace;\n",
            "y la luz me da, y la luz me da.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Palabra = 'AGOTADO'\n",
        "text = Generate2(model.to('cpu'),Palabra)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "QoXmojQ-pk7z",
        "outputId": "e9804d9e-9bad-4511-8a58-3348b982ebe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGOTADO\n",
            "\n",
            "\n",
            "Por ejemplo, si, y si\n",
            "empuja,\n",
            "no sé si se\n",
            "\n",
            "tiende a tener fe,\n",
            "si no la tiene,\n",
            "no sé si se\n",
            "cruza de los dedos.? (Nadie lo sabe. Es la voz de un muerto).\n",
            "\n",
            "Pierde la mañana entera en la\n",
            "conjunción de que fue muerta. Es la tristeza la que habla en el silencio. Y entonces me acuerdo del funeral de alguien que la vio morir... ¡la misma muerte!... la misma noche que la muerte... Le pregunté a la vieja, que me creía. Le respondieron que la muerta era la misma muerte. No puedo creer que haya sido tan triste... Le comenté a la vieja que no, pero que le dije que no era la muerte. Le dije que no, que era la muerte, sino la muerte. Le comenté que no. Le comenté que no\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "94e7570899995adebab4ebab5cd3752e227f734c99b4f5f3f0d280f8bef09b63"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('IA')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Gasolina.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}